# Local Data Lakehouse: Spark (Windows), SQL Server & AWS Iceberg

Este projeto documenta e implementa um pipeline de dados distribuÃ­do (ETL) executado em um **cluster local Apache Spark (Standalone)** no Windows. O objetivo Ã© demonstrar a extraÃ§Ã£o paralela de alto desempenho do SQL Server e a carga em tabelas **Apache Iceberg** no Amazon S3, catalogadas via AWS Glue.

![Estrutura do projeto](img/projeto_spark.png)

Este repositÃ³rio contÃ©m todos os binÃ¡rios necessÃ¡rios (`winutils`, `.dll`) para facilitar a configuraÃ§Ã£o em ambiente Windows.

## ðŸ— Arquitetura

O fluxo de dados segue a arquitetura abaixo:

```mermaid
graph LR
    SQL[SQL Server] -->|JDBC Parallel Read| Spark["Spark Cluster Local (Windows Standalone)"]
    Spark -->|Write Parquet| LocalStaging["Local Disk (Staging)"]
    LocalStaging -->|Write Iceberg| S3[AWS S3 Bucket]
    Glue[AWS Glue Catalog] -.->|Metastore| S3

```

## ðŸ“‹ PrÃ©-requisitos e DependÃªncias

Para reproduzir este laboratÃ³rio, seu ambiente deve atender estritamente Ã s seguintes versÃµes para evitar incompatibilidades conhecidas:

1. **Sistema Operacional:** Windows 10 ou 11.
2. **Java:** JDK 8 ou 11 (Recomendado JDK 11 para Spark 3.5).
3. **Python:** **VersÃ£o 3.11** (ObrigatÃ³rio).
> âš ï¸ **AtenÃ§Ã£o:** NÃ£o utilize Python 3.12 ou superior. Existe um bug conhecido de incompatibilidade com o Apache Spark 3.5 que causa *crashes* silenciosos (SegFaults) durante a execuÃ§Ã£o dos executors.


4. **Apache Spark:** VersÃ£o 3.5.7.

## âš™ï¸ InstalaÃ§Ã£o e ConfiguraÃ§Ã£o

Este repositÃ³rio jÃ¡ inclui dependÃªncias crÃ­ticas para o funcionamento do Hadoop no Windows e autenticaÃ§Ã£o do SQL Server.

### 1. Configurar Hadoop (Winutils)

O Spark no Windows precisa de binÃ¡rios nativos do Hadoop para operaÃ§Ãµes de I/O.

1. Localize a pasta `hadoop` neste repositÃ³rio.
2. Copie a pasta para um local fixo, ex: `C:\hadoop`.
3. Defina a variÃ¡vel de ambiente:
* `HADOOP_HOME = C:\hadoop`


4. Adicione ao **PATH** do Windows: `%HADOOP_HOME%\bin`.

### 2. Configurar AutenticaÃ§Ã£o SQL Server

Para usar `integratedSecurity=true` (AutenticaÃ§Ã£o do Windows) via JDBC:

1. Localize o arquivo `mssql-jdbc_auth.dll` na pasta `libs` ou `hadoop/bin` deste repositÃ³rio.
2. Copie este arquivo para `C:\Windows\System32` **OU** certifique-se de que a pasta onde ele estÃ¡ esteja no seu **PATH**.

### 3. Configurar Apache Spark

1. Baixe e extraia o Apache Spark 3.5.7.
2. Defina `SPARK_HOME` apontando para a pasta extraÃ­da.
3. Adicione `%SPARK_HOME%\bin` ao **PATH**.

### 4. ConfiguraÃ§Ã£o do `spark-defaults.conf`

Navegue atÃ© `%SPARK_HOME%\conf`, renomeie `spark-defaults.conf.template` para `spark-defaults.conf` e substitua o conteÃºdo pelo abaixo.

**Destaques da ConfiguraÃ§Ã£o:**

* **IntegraÃ§Ã£o Iceberg/AWS:** Os pacotes (`spark.jars.packages`) baixam automaticamente as dependÃªncias do Iceberg, AWS SDK e conector MSSQL.
* **OtimizaÃ§Ã£o Local:** Uso de `KryoSerializer` e ajuste de memÃ³ria para rodar em desktop.
* **Monitoramento:** Logs de eventos ativados para o Spark History Server.

```properties
# --- Performance e SerializaÃ§Ã£o ---
spark.serializer                     org.apache.spark.serializer.KryoSerializer
spark.kryoserializer.buffer.max      512m
spark.sql.adaptive.enabled           true
spark.sql.adaptive.coalescePartitions.enabled true
spark.sql.shuffle.partitions         500

# --- Recursos do Cluster (Ajuste conforme sua RAM) ---
spark.executor.instances             2
spark.executor.cores                 4
spark.executor.memory                5g
spark.driver.memory                  2g
spark.memory.fraction                0.8

# --- Rede e Master Local ---
spark.master                         spark://192.168.59.62:7077
spark.master.port                    7077
spark.master.webui.port              8080
spark.worker.ui.port                 8081
spark.driver.host                    192.168.59.62
spark.driver.bindAddress             0.0.0.0
spark.network.timeout                800s
spark.executor.heartbeatInterval     60s

# --- Pacotes (Iceberg, AWS, Hadoop, MSSQL) ---
# O Spark baixarÃ¡ estes JARs automaticamente na primeira execuÃ§Ã£o
spark.jars.packages                  org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.10.0,org.apache.iceberg:iceberg-aws-bundle:1.10.0,org.apache.hadoop:hadoop-aws:3.3.4,org.apache.hadoop:hadoop-common:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.764,com.microsoft.sqlserver:mssql-jdbc:13.2.1.jre11

# --- ConfiguraÃ§Ã£o do CatÃ¡logo Iceberg (AWS Glue) ---
spark.sql.defaultCatalog             dev
spark.sql.catalog.dev                org.apache.iceberg.spark.SparkCatalog
spark.sql.catalog.dev.type           glue
spark.sql.catalog.dev.warehouse      s3://data-warehouse/tables/
spark.sql.catalog.dev.io-impl        org.apache.iceberg.aws.s3.S3FileIO
spark.sql.catalog.dev.s3.cross-region-access-enabled true
spark.sql.catalog.dev.glue.skip-name-validation      true
spark.sql.extensions                 org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions

# --- OtimizaÃ§Ã£o S3 ---
spark.hadoop.fs.s3.impl              org.apache.hadoop.fs.s3a.S3AFileSystem
spark.hadoop.fs.s3a.impl             org.apache.hadoop.fs.s3a.S3AFileSystem
spark.hadoop.fs.s3a.fast.upload      true
spark.hadoop.fs.s3a.multipart.size   100M
spark.sql.parquet.int96RebaseModeInWrite CORRECTED

# --- Logs e History Server ---
spark.eventLog.enabled               true
spark.eventLog.dir                   file:///C:/spark/spark-3.5.7-bin-hadoop3/spark-events
spark.history.fs.logDirectory        file:///C:/spark/spark-3.5.7-bin-hadoop3/spark-events
spark.history.ui.port                18080

```

---

## ðŸš€ ExecuÃ§Ã£o do Pipeline

### 1. Iniciar o Cluster

Abra o terminal (Powershell ou CMD) e inicie o Master e o Worker:

```powershell
# Iniciar Master
spark-class org.apache.spark.deploy.master.Master

# Em outro terminal, Iniciar Worker (aponte para o IP do seu master)
spark-class org.apache.spark.deploy.worker.Worker spark://192.168.59.62:7077

```

### 2. Iniciar o History Server (Opcional, para monitoramento)

```powershell
spark-class org.apache.spark.deploy.history.HistoryServer

```

Acesse `http://localhost:18080` para ver os logs de execuÃ§Ã£o.

### 3. Executar o Job Python

Certifique-se de que suas credenciais AWS estÃ£o configuradas (variÃ¡veis de ambiente ou `~/.aws/credentials`).

```bash
python main_pipeline.py

```

---

## ðŸ ExplicaÃ§Ã£o do CÃ³digo (`main_pipeline.py`)

O script Python Ã© modularizado para garantir robustez e performance. Abaixo, a explicaÃ§Ã£o detalhada de cada componente.

### 1. Classes de Dados (`SparkTable` e `URLMssql`)

Utilizamos `dataclasses` para evitar "hardcoding" de strings e facilitar a manutenÃ§Ã£o.

* **`SparkTable`**: Define a estrutura da tabela (servidor, banco, schema, chaves). Gera automaticamente os nomes qualificados para SQL Server e AWS Athena/Glue.
* **`URLMssql`**: ConstrÃ³i a string de conexÃ£o JDBC complexa, injetando configuraÃ§Ãµes de seguranÃ§a (`integratedSecurity=true`) e performance.

### 2. Tratamento DinÃ¢mico de Datas (`parse_date_expressions`)

Permite usar placeholders na string de condiÃ§Ã£o, facilitando agendamentos (Airflow/Cron) sem alterar o cÃ³digo.

* **Funcionalidade:** Substitui termos como `{hoje}`, `{ontem}`, `{inicio_mes}` e operaÃ§Ãµes aritmÃ©ticas (ex: `{hoje-3d}`) pelas datas reais no momento da execuÃ§Ã£o.

### 3. Paralelismo Inteligente (`lower_upper_bound` e `iter_lower_upper_bound`)

O gargalo do JDBC Ã© ler tudo em uma Ãºnica thread. Estas funÃ§Ãµes resolvem isso:

* **`lower_upper_bound`**: Consulta o `MIN` e `MAX` da chave primÃ¡ria (PK) na origem. Esses valores alimentam as opÃ§Ãµes `lowerBound` e `upperBound` do Spark JDBC, permitindo que o Spark divida a leitura em N partiÃ§Ãµes simultÃ¢neas (definido por `numPartitions`).
* **`iter_lower_upper_bound`**: Para tabelas muito grandes, quebra a leitura em "lotes" (chunks) lÃ³gicos, evitando sobrecarregar a memÃ³ria do driver ou do banco de dados.

### 4. ExtraÃ§Ã£o e Staging (`write_parquet`)

* LÃª do SQL Server usando as partiÃ§Ãµes calculadas.
* Normaliza colunas para minÃºsculo (boa prÃ¡tica para Data Lakes).
* Escreve em disco local (`LOCAL_FILES`) em formato **Parquet** com compressÃ£o **ZSTD**. Isso serve como uma Ã¡rea de *staging* segura antes do upload para nuvem.

### 5. Carga no Data Lake (`overwrite_table_iceberg`)

* LÃª os arquivos Parquet locais.
* Escreve na tabela Iceberg no S3 (`glue.db.table`).
* **Propriedades CrÃ­ticas:**
* `write.format.default`: Parquet.
* `write.merge.mode`: **merge-on-read** (Ideal para atualizaÃ§Ãµes frequentes, grava deltas rapidamente).
* `write.target-file-size-bytes`: 128MB (Otimizado para leitura de engines como Trino/Athena).



### 6. ManutenÃ§Ã£o AutomÃ¡tica (`optimize_table`)

O Iceberg requer manutenÃ§Ã£o para nÃ£o degradar a performance (problema de "small files"). O script executa automaticamente ao final:

1. **`rewrite_data_files`**: Compacta arquivos pequenos em arquivos maiores (bin-packing).
2. **`expire_snapshots`**: Remove versÃµes antigas da tabela (time travel) para economizar espaÃ§o no S3.
3. **`remove_orphan_files`**: Limpa arquivos que nÃ£o estÃ£o mais referenciados em nenhum snapshot.

---

## ðŸ“„ CÃ³digo Completo

```python
from datetime import datetime, timedelta
from dataclasses import dataclass
import uuid
import re
import shutil
from pyspark.sql import SparkSession

LOCAL_FILES = "M:/lake_house"

@dataclass
class SparkTable:
    server: str
    database: str
    schema_table: str
    table_name: str
    primary_key: str
    conds: str = None

    @property
    def full_name(self) -> str:
        return f"{self.database}.{self.schema_table}.{self.table_name}"

    @property
    def full_name_athena(self) -> str:
        return f"{self.database.lower()}_{self.schema_table.lower()}_{self.table_name.lower()}"

@dataclass
class URLMssql:
    table: SparkTable
    driver: str = "com.microsoft.sqlserver.jdbc.SQLServerDriver"
    isolation_level: str = "READ_UNCOMMITTED"
    prefer_timestamp_ntz: bool = True

    @property
    def full_url(self) -> str:
        return (
            f"jdbc:sqlserver://{self.table.server}:1433;"
            f"databaseName={self.table.database};"
            "encrypt=true;integratedSecurity=true;trustServerCertificate=true;"
        )

    @property
    def options(self) -> dict[str, str]:
        return {
            "url": self.full_url,
            "driver": self.driver,
            "isolationLevel": self.isolation_level,
            "preferTimestampNTZ": self.prefer_timestamp_ntz,
        }

def parse_date_expressions(query_string: str | None) -> str | None:
    if not query_string:
        return query_string

    hoje = datetime.now()
    bases = {
        "hoje": hoje,
        "ontem": hoje - timedelta(days=1),
        "inicio_mes": hoje.replace(day=1),
        "inicio_ano": hoje.replace(month=1, day=1),
    }

    def replace_match(match: re.Match):
        base_name = match.group(1)
        operator = match.group(2)
        amount = match.group(3)

        target_date = bases.get(base_name)
        if not target_date:
            return match.group(0)

        if not operator:
            return target_date.strftime("%Y-%m-%d")

        days = int(amount)
        if operator == "-":
            new_date = target_date - timedelta(days=days)
        elif operator == "+":
            new_date = target_date + timedelta(days=days)

        return new_date.strftime("%Y-%m-%d")

    pattern = r"\{(hoje|ontem|inicio_mes|inicio_ano)(?:([\+\-])(\d+)d)?\}"
    return re.sub(pattern, replace_match, query_string)

def lower_upper_bound(spark: SparkSession, table: SparkTable) -> tuple:
    url = URLMssql(table)
    stmt = f"""
    select 
        min({table.primary_key}) as min_lower, 
        max({table.primary_key}) as max_upper
    from {table.full_name} with(nolock)
    {f"where {table.conds}" if table.conds else ""}
    """
    options = url.options
    options |= {"query": stmt}
    start, end = spark.read.format("jdbc").options(**options).load().collect()[0]
    return start, end

def write_parquet(spark: SparkSession, table: SparkTable, cores: int = 8) -> str:
    start, end = lower_upper_bound(spark, table)
    url = URLMssql(table)

    if isinstance(start, datetime):
        start, end = f"{start:%Y-%m-%d %H:%M:%S}", f"{end:%Y-%m-%d %H:%M:%S}"

    options = url.options
    options |= {
        "dbtable": f"(select * from {table.full_name} with(nolock) {f'where {table.conds}' if table.conds else ''}) as tabela",
        "partitionColumn": table.primary_key,
        "numPartitions": cores,
        "fetchsize": 100_000,
        "lowerBound": f"{start}",
        "upperBound": f"{end}",
    }

    df = spark.read.format("jdbc").options(**options).load()
    rename_cols = {c: c.lower() for c in df.columns}

    df.withColumnsRenamed(rename_cols).write.parquet(
        f"{LOCAL_FILES}/{table.full_name_athena}/{table.full_name_athena}_{uuid.uuid4()}",
        compression="zstd",
        mode="overwrite",
    )
    return table

def overwrite_table_iceberg(spark: SparkSession, table: SparkTable, schema: str = "spark_load") -> SparkTable:
    df = spark.read.parquet(f"{LOCAL_FILES}/{table.full_name_athena}/{table.full_name_athena}_*")
    (
        df.coalesce(24)
        .writeTo(f"{schema}.{table.full_name_athena}")
        .tableProperty("write.format.default", "parquet")
        .tableProperty("write.parquet.compression-codec", "zstd")
        .tableProperty("write.parquet.compression-level", "3")
        .tableProperty("write.distribution-mode", "none")
        .tableProperty("write.object-storage.enabled", "true")
        .tableProperty("write.target-file-size-bytes", "134217728")
        .tableProperty("write.delete.mode", "merge-on-read")
        .tableProperty("write.update.mode", "merge-on-read")
        .tableProperty("write.merge.mode", "merge-on-read")
        .createOrReplace()
    )
    return table

def optimize_table(spark: SparkSession, table: SparkTable, schema: str = "spark_load") -> SparkTable:
    print(" => rewrite_data_files")
    table_name = f"{schema}.{table.full_name_athena}"
    spark.sql(f"""
        CALL system.rewrite_data_files(
            table => '{table_name}',
            options => map('partial-progress.enabled', 'true', 'rewrite-all', 'true')
        )
    """)
    
    print(" => expire_snapshots")
    dt_expire = datetime.now() - timedelta(minutes=1)
    spark.sql(f"""
        CALL system.expire_snapshots(
            table => '{table_name}',
            older_than => TIMESTAMP '{dt_expire:%Y-%m-%d %H:%M:%S}',
            retain_last => 1
        )
    """)
    
    print(" => remove_orphan_files")
    spark.sql(f"""
        CALL system.remove_orphan_files(
            table => '{table_name}',
            dry_run => false
        )
    """)
    return table

def iter_lower_upper_bound(spark: SparkSession, table: SparkTable, chunk: int = 100) -> list[tuple]:
    url = URLMssql(table)
    stmt = f"""
    select distinct {table.primary_key} as chave
    from {table.full_name} with(nolock)
    {f"where {table.conds}" if table.conds else ""}
    """
    options = url.options
    options |= {"query": stmt}
    rows = spark.read.format("jdbc").options(**options).load().collect()
    rows_sorted = sorted([row.chave for row in rows])
    lista_batch = []
    for idx in range(0, len(rows_sorted), chunk):
        lotes = rows_sorted[idx : idx + chunk]
        lista_batch.append((lotes[0], lotes[-1]))
    return lista_batch

def main_spark_jdbc(
    server: str,
    database: str,
    schema_table: str,
    table_name: str,
    primary_key: str,
    conds: str | None = None,
    chunk: int = 100,
    cores: int = 8,
    optimize: bool = False,
    iter_lower: bool = True,
    create_table: bool = True,
    drop_table: bool = False,
) -> None:
    try:
        spark: SparkSession = SparkSession.builder.appName("job_jdbc").getOrCreate()
        conds = parse_date_expressions(conds)
        tbl = SparkTable(server, database, schema_table, table_name, primary_key, conds)

        if iter_lower:
            rows = iter_lower_upper_bound(spark, tbl, chunk=chunk)
            where = f"and ({conds})" if conds else ""
            for start, end in rows:
                tbl.conds = f"{tbl.primary_key} between {start} and {end} {where}"
                print(tbl.conds)
                print("1 - WRITE PARQUET")
                write_parquet(spark, tbl, cores)
        else:
            write_parquet(spark, tbl, cores)

        if create_table:
            print("2 - OVERWRITE TABLE ICEBERG")
            overwrite_table_iceberg(spark, tbl)

        if optimize and not drop_table:
            print("5 - OPTIMIZE TABLE")
            optimize_table(spark, tbl)
    except Exception as e:
        print(e)
        raise
    finally:
        shutil.rmtree(f"{LOCAL_FILES}/{tbl.full_name_athena}", ignore_errors=True)
        spark.stop()

```